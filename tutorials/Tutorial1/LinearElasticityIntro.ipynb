{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Elasticity Deterministic Inverse Problem\n",
    "\n",
    "This notebook uses `hippylib` to set up the parametric PDE map and eventually solve a deterministic inverse problem for an uncertain elastic modulus in a linear elasticity problem\n",
    "\n",
    "\n",
    "## Linear elasticity\n",
    "\n",
    "We can derive linear elasticity as the minimization of strain energy\n",
    "\n",
    "$$\n",
    "\\min_u \\int_\\Omega \\psi(\\varepsilon(u)) dx - \\int_\\Omega f\\cdot u dx - \\int_{\\Gamma_N} t \\cdot n ds(x)\n",
    "$$\n",
    "where $\\varepsilon(u) = \\frac{1}{2}(\\nabla u + \\nabla u^T)$\n",
    "where $u$ is restricted to an appropriate space to satisfy given Dirichlet boundary conditions, $f$ is a body load, and $t$ is a prescribed traction condition. Additionally:\n",
    "\n",
    "$$\n",
    "\\psi(\\boldsymbol{\\varepsilon}) = \\frac{\\lambda}{2} \\bigl(\\mathrm{tr}(\\boldsymbol{\\varepsilon})\\bigr)^2 + \\mu \\,\\mathrm{tr}(\\boldsymbol{\\varepsilon}^2)\n",
    "$$\n",
    "\n",
    "The minimizer gives us the following system\n",
    "\n",
    "$$ -\\nabla \\cdot \\sigma(u) = f$$\n",
    "$$ u = u_D \\text{ on } \\Gamma_D$$\n",
    "$$ \\sigma \\cdot n = t \\text{ on } \\Gamma_N$$\n",
    "\n",
    "$$ \\sigma(u) = \\lambda (\\nabla \\cdot u) I + 2\\mu \\varepsilon(u)$$\n",
    "\n",
    "## Parametric PDE Mapping\n",
    "\n",
    "In our case we parametrize our model by the elastic modulus:\n",
    "\n",
    "$$ E = \\frac{\\mu(3\\lambda + 2\\mu)}{\\lambda + \\mu} $$\n",
    "\n",
    "and we assume the Poisson's ration $\\nu = \\frac{\\lambda}{2(\\lambda + \\mu)} = 0.4$ to be fixed.\n",
    "\n",
    "We generate training data samples of $E$ using Gaussian random fields $m$. We assume $E = 1 + \\exp(m)$, in order to maintain the coercivity (well-posedness) of the PDE.\n",
    "\n",
    "## Operator Learning Primary Goal\n",
    "\n",
    "**We are interested in the mapping from $m$ to the associated displacement field $u(m)$ over a given distribution of $m$**\n",
    "\n",
    "$$ m \\mapsto u(m) \\quad \\text{ over } \\quad m \\sim \\mu$$\n",
    "\n",
    "\n",
    "## Finite Element Method\n",
    "\n",
    "For all tutorials we will utilize the finite element method through the open source library `fenics`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIT License\n",
    "# Copyright (c) 2025\n",
    "#\n",
    "# This is part of the dino_tutorial package\n",
    "# \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in\n",
    "# all copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND.\n",
    "# For additional questions contact Thomas O'Leary-Roseberry\n",
    "\n",
    "import dolfin as dl\n",
    "import ufl\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os\n",
    "import time\n",
    "sys.path.append( os.environ.get('HIPPYLIB_PATH', \"../\") )\n",
    "import hippylib as hp\n",
    "\n",
    "from linear_elasticity_model import *\n",
    "\n",
    "# sys.path.append( os.environ.get('HIPPYFLOW_PATH'))\n",
    "# import hippyflow as hf\n",
    "\n",
    "import logging\n",
    "logging.getLogger('FFC').setLevel(logging.WARNING)\n",
    "logging.getLogger('UFL').setLevel(logging.WARNING)\n",
    "dl.set_log_active(False)\n",
    "\n",
    "np.random.seed(seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Objects\n",
    "\n",
    "- `settings` defines auxiliary hyperparameters for the PDE problem\n",
    "- `hippylib.model` is an object that contains all of the sub-objects that define the PDE problem and the associated inverse problem. Within the model we have\n",
    "    - `hippylib.model.prior` defines the reference distribution for the parameter. We will utilize Gaussian priors, as they allow for the definition of infinite-dimensionally consistent measures for spatially correlated random variables.\n",
    "        $$ m\\sim \\mu = \\mathcal{N}(\\overline{m},\\mathcal{C})$$ \n",
    "    - `hippylib.model.problem` defines the PDE problem variationally using the finite element method. This contains all of the infrastructure for solving the PDE, including the solver (linear and nonlinear) as well as additional methods that will be utilized to compute derivatives later on.\n",
    "      $$ \\text{Given } m \\sim \\mu, \\quad \\text{evaluate} \\quad m\\mapsto u(m)$$\n",
    "\n",
    "\n",
    "### Note on extensibility\n",
    "\n",
    "All of the code is written to intentionally abstract away the PDE model and reference distribution. This is to make it easier to rapid-prototype other PDE problems later with a BYO-PDE problem.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some basic settings for \n",
    "settings = linear_elasticity_settings()\n",
    "# Define model\n",
    "model = linear_elasticity_model(settings)\n",
    "\n",
    "# This is a python list of three function spaces\n",
    "# [Vh_STATE, Vh_PARAMETER, Vh_ADJOINT]\n",
    "Vh = model.problem.Vh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The prior / reference distribution $\\mu$\n",
    "\n",
    "We seek to model the elastic modulus field as a nonlinear function of a Gaussian random field.\n",
    "    $$ E(m) = 1 + \\exp(m) $$\n",
    "\n",
    "### Gaussian random fields\n",
    "\n",
    "We model $m$ as a spatially correlated random field. Towards this end we utilize a self-adjoint fractional elliptic PDE operator for the covariance to impart local spatial coupling.\n",
    "\n",
    "$$ m \\sim \\mathcal{N}(\\overline{m},\\mathcal{C}) $$\n",
    "\n",
    "$$ \\text{Mean function:} \\quad \\overline{m} $$\n",
    "\n",
    "$$ \\text{Covariance:} \\quad \\mathcal{C} = (I - \\Delta)^{-\\alpha} \\quad \\text{+ B.C.s}$$ \n",
    "\n",
    "Sampling from Gaussians: Given white noise $\\xi$, then \n",
    "$$ m = \\overline{m} + \\mathcal{C}^\\frac{1}{2} \\xi \\sim \\mathcal{N}(\\overline{m},\\mathcal{C})$$\n",
    "\n",
    "So we need to be able to apply $\\mathcal{C}^\\frac{1}{2}$. Choosing $\\alpha > \\frac{d}{2}$ where $\\Omega \\subset \\mathbb{R}^d$ leads to **discretization-consistency**. The choice of $\\alpha = 2$ is suitable for discretization consistency for both $\\mathbb{R}^2$ and $\\mathbb{R}^3$ and is utilized commonly. Thus the application of $\\mathcal{C}^\\frac{1}{2}$ corresponds to a PDE solve:\n",
    "\n",
    "$$ (I - \\Delta)^{-1}.$$\n",
    "\n",
    "This construction costs for this elliptic PDE solution operator can be amortized over its application to many right-hand sides. This allows for substantial savings when using sparse solvers or preconditioners. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data:\n",
    "############\n",
    "noise = dl.Vector()\n",
    "model.prior.init_vector(noise,\"noise\")\n",
    "hp.parRandom.normal(1., noise)\n",
    "mtrue = dl.Vector()\n",
    "model.prior.init_vector(mtrue, 0)\n",
    "model.prior.sample(noise, mtrue)\n",
    "\n",
    "# # plot mtrue, mean \n",
    "# objs = [dl.Function(model.problem.Vh[hp.PARAMETER],mtrue), \n",
    "#         dl.Function(model.problem.Vh[hp.PARAMETER],model.prior.mean)]\n",
    "# mytitles = [\"True Parameter\", \"Prior mean\"]\n",
    "# hp.utils.nb.multi1_plot(objs, mytitles)\n",
    "# plt.show()\n",
    "\n",
    "cbar = dl.plot(dl.Function(model.problem.Vh[hp.PARAMETER],model.prior.mean), title=\"Mean field\")\n",
    "# _ = plt.colorbar(cbar)\n",
    "# plt.show()\n",
    "\n",
    "cbar = dl.plot(dl.Function(model.problem.Vh[hp.PARAMETER],mtrue), title=\"A sample\")\n",
    "# _ = plt.colorbar(cbar)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve the PDE and look at the associated displacement field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "utrue = model.problem.generate_state()\n",
    "x = [utrue, mtrue, None]\n",
    "\n",
    "model.problem.solveFwd(x[hp.STATE], x)\n",
    "\n",
    "plt.figure(figsize=(15,11))\n",
    "u_norm = get_unorm(model, utrue)\n",
    "\n",
    "u_plot = dl.Function(Vh[hp.STATE])\n",
    "u_plot.vector().zero()\n",
    "u_plot.vector().axpy(1.,utrue)\n",
    "\n",
    "vmax = u_norm.vector().max()\n",
    "vmin = u_norm.vector().min()\n",
    "\n",
    "# Project onto a scalar function space. Often, u0.function_space() is a scalar subspace.\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(221)\n",
    "cbar = dl.plot(u_plot, mode=\"displacement\", title=\"Displacement\")\n",
    "_ = plt.colorbar(cbar)\n",
    "\n",
    "\n",
    "# hp.utils.nb.plot(u_norm, mytitle=\"Absolute Value (u)\", subplot_loc=221, vmin=vmin, vmax=vmax)\n",
    "# norm = np.linalg.norm(model.misfit.d.get_local().reshape((-1, 2)), axis=1)\n",
    "# tmp = dl.Vector(dl.MPI.comm_world, len(norm))\n",
    "# tmp.set_local(norm)\n",
    "# hp.utils.nb.plot_pts(model.targets, tmp, mytitle=\"Observations (u)\", subplot_loc=222, vmin=vmin, vmax=vmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.plot(u_plot, mode=\"displacement\", title=\"Displacement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse Problem\n",
    "\n",
    "We have pointwise observations of the displacements $u \\in \\mathcal{U}$. The observation operator is $B:\\mathcal{U} \\rightarrow \\mathbb{R}^{d}$. The observational data $\\mathbf{d}$ are assumed to agree with the true observations up to some irreducible observational noise $\\xi \\sim \\mathcal{N}(0,\\Gamma)$. That is $\\mathbf{d} = Bu(m_\\text{true}) + \\xi$\n",
    "\n",
    "The minimization problem is \n",
    "\n",
    "$$ \\min_m \\frac{1}{2}\\|Bu(m) - \\mathbf{d}\\|^2_{\\Gamma^{-1}} + \\frac{1}{2}\\|m - \\overline{m}\\|^2_{\\mathcal{C}^{-1}}$$\n",
    "\n",
    "where $\\mathcal{C}^{-1}$ is a weighting for a Tikhonov regularization, and in the Bayesian context we identify it with the covariance of a Gaussian prior, e.g., $\\mu_\\text{prior} = \\mathcal{N}(\\overline{m},\\mathcal{C})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation operator $B$ and observational data $\\mathbf{d}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hippylib.utils.nb as nb\n",
    "\n",
    "data = model.misfit.B*x[hp.STATE]\n",
    "# MAX = data.norm(\"linf\")\n",
    "# noise_std_dev = rel_noise * MAX\n",
    "hp.parRandom.normal_perturb(settings['noise_variance']**0.5, data)\n",
    "model.misfit.d = data\n",
    "\n",
    "vmax = max( utrue.max(), model.misfit.d.max() )\n",
    "vmin = min( utrue.min(), model.misfit.d.min() )\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "nb.plot(dl.Function(model.problem.Vh[hp.STATE], utrue), mytitle=\"True State\", subplot_loc=121, vmin=vmin, vmax=vmax)\n",
    "norm = np.linalg.norm(model.misfit.d.get_local().reshape((-1, 2)), axis=1)\n",
    "tmp = dl.Vector(dl.MPI.comm_world, len(norm))\n",
    "tmp.set_local(norm)\n",
    "nb.plot_pts(model.targets, tmp, mytitle=\"Observed Data $\\mathbf{d}$\", subplot_loc=122, vmin=vmin, vmax=vmax)\n",
    "\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve the nonlinear least-squares problem using Newton\n",
    "\n",
    "Define the objective function\n",
    "\n",
    "$$ F(m) = \\frac{1}{2}\\|Bu(m) - \\mathbf{d}\\|^2_{\\Gamma^{-1}} + \\frac{1}{2}\\|m - \\overline{m}\\|^2_{\\mathcal{C}^{-1}}$$\n",
    "\n",
    "We can then compute gradients $\\nabla F(m)$, and Hessians $\\nabla^2 F(m)$, and minimize $F(m)$ utilizing a Newton method\n",
    "\n",
    "$$ m_{k+1} = m_k +\\alpha_k p_k$$\n",
    "$$ p_k = -\\nabla^2 F(m_k)^{-1}\\nabla F(m_k)$$\n",
    "\n",
    "The derivatives are typically computed using **adjoint methods** since they require differentiating through an implicitly defined PDE solution operator. The costs of computing (implicit) derivatives are then measured in PDE solutions.\n",
    "\n",
    "- each gradient requires an additional (linearized) adjoint PDE solution.\n",
    "- each Hessian vector product requires two additional linearized PDE solutions (fwd incremental PDE and adjoint incremental PDE).\n",
    "\n",
    "We can avoid explicitly inverting the Hessian by utilizing an inexact Newton conjugate gradients method. Additionally we can utilize a stopping criterion for the linearized solve via the so-called Eisenstat--Walker conditions\n",
    "\n",
    "$$ \\|\\nabla^2 F(m_k)p_k + \\nabla F(m_k)\\| \\leq c\\|\\nabla F(m_k)\\| $$\n",
    "\n",
    "so we require that the linear solves are asymptotically exact as $\\|\\nabla F(m_k)\\| \\rightarrow 0$, while allowing for computational savings far from the solution $\\|\\nabla F(m^\\star)\\| = 0$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute MAP point\n",
    "m = model.prior.mean.copy()\n",
    "solver = hp.ReducedSpaceNewtonCG(model)\n",
    "solver.parameters[\"rel_tolerance\"] = 1e-6\n",
    "solver.parameters[\"abs_tolerance\"] = 1e-12\n",
    "solver.parameters[\"max_iter\"]      = 100\n",
    "solver.parameters[\"GN_iter\"] = 20\n",
    "solver.parameters[\"globalization\"] = \"LS\"\n",
    "solver.parameters[\"LS\"][\"c_armijo\"] = 1e-4\n",
    "\n",
    "    \n",
    "x = solver.solve([None, m, None])\n",
    "    \n",
    "if solver.converged:\n",
    "    print( \"\\nConverged in \", solver.it, \" iterations.\")\n",
    "else:\n",
    "    print( \"\\nNot Converged\")\n",
    "\n",
    "print( \"Termination reason: \", solver.termination_reasons[solver.reason] )\n",
    "print( \"Final gradient norm: \", solver.final_grad_norm )\n",
    "print( \"Final cost: \", solver.final_cost )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot MAP point\n",
    "plt.figure(figsize=(15,5))\n",
    "nb.plot(dl.Function(model.problem.Vh[hp.PARAMETER],mtrue), subplot_loc=121,mytitle=\"True Parameter\")\n",
    "nb.plot(dl.Function(model.problem.Vh[hp.PARAMETER], x[hp.PARAMETER]), subplot_loc=122,mytitle=\"MAP Parameter\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra credit! Let's look at the Hessian at the optimum $\\nabla^2F(m^\\star)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.setPointForHessianEvaluations(x, gauss_newton_approx=False)\n",
    "Hmisfit = hp.ReducedHessian(model, misfit_only=True)\n",
    "k = 50\n",
    "p = 20\n",
    "print( \"Single/Double Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}.\".format(k,p) )\n",
    "\n",
    "Omega = hp.MultiVector(x[hp.PARAMETER], k+p)\n",
    "hp.parRandom.normal(1., Omega)\n",
    "lmbda, V = hp.doublePassG(Hmisfit, model.prior.R, model.prior.Rsolver, Omega, k)\n",
    "\n",
    "posterior = hp.GaussianLRPosterior(model.prior, lmbda, V)\n",
    "posterior.mean = x[hp.PARAMETER]\n",
    "\n",
    "plt.plot(range(0,k), lmbda, 'b*', range(0,k+1), np.ones(k+1), '-r')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('number')\n",
    "plt.ylabel('eigenvalue')\n",
    "\n",
    "nb.plot_eigenvectors(model.problem.Vh[hp.PARAMETER], V, mytitle=\"Eigenvector\", which=[0,1,2,5,10,15])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
